{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11631813,"sourceType":"datasetVersion","datasetId":7297994}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nfrom transformers import (\n    set_seed,\n    RobertaTokenizerFast,\n    RobertaForSequenceClassification,\n    DataCollatorWithPadding,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback,\n)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    classification_report,\n    confusion_matrix,\n)\n\n# Disable WANDB\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# 1. Reproducibility\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\nset_seed(SEED)                               # Python, NumPy, PyTorch\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# 2. Load data without dropping rows; fill missing, cast to str\ntrain_df = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/train.csv\", encoding=\"latin1\")\ntest_df  = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/test.csv\",  encoding=\"latin1\")\n\nfor df in (train_df, test_df):\n    for col in [\"Question\",\"CorrectAnswer\",\"Response\"]:\n        df[col] = df[col].fillna(\"\").astype(str)\n\n# 3. Map labels {-1→0, 0→1, 1→2}\nlabel_map = {-1:0, 0:1, 1:2}\ntrain_df[\"label\"] = train_df[\"label\"].map(label_map)\ntest_df[\"label\"]  = test_df[\"label\"].map(label_map)\n\n# 4. Stratified train/validation split (80/20)\ntrain_data, val_data = train_test_split(\n    train_df, test_size=0.2, stratify=train_df[\"label\"], random_state=SEED\n)\n\n# 5. Tokenizer, model, collator\nMODEL_NAME = \"roberta-base\"\ntokenizer  = RobertaTokenizerFast.from_pretrained(\n    MODEL_NAME, add_prefix_space=True\n)\nmodel      = RobertaForSequenceClassification.from_pretrained(\n    MODEL_NAME, num_labels=3\n)\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# 6. Dataset using pair-encoding API\nclass ShortAnswerDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        enc = self.tokenizer(\n            row.Question + \" \" + row.CorrectAnswer,\n            row.Response,\n            truncation=True,\n            max_length=self.max_len\n        )\n        enc[\"labels\"] = torch.tensor(row.label, dtype=torch.long)\n        return enc\n\ntrain_ds = ShortAnswerDataset(train_data, tokenizer)\nval_ds   = ShortAnswerDataset(val_data,   tokenizer)\ntest_ds  = ShortAnswerDataset(test_df,    tokenizer)\n\n# 7. Weighted sampler for class imbalance\ncounts = train_data[\"label\"].value_counts().sort_index()\nclass_weights = 1.0 / counts\nsample_weights = train_data[\"label\"].map(class_weights).values\ntrain_sampler = WeightedRandomSampler(\n    weights=sample_weights,\n    num_samples=len(sample_weights),\n    replacement=True\n)\n\n# 8. Focal Loss\nclass FocalLoss(torch.nn.Module):\n    def __init__(self, gamma=2.0, alpha=1.0):\n        super().__init__()\n        self.gamma = gamma\n        self.alpha = alpha\n\n    def forward(self, logits, targets):\n        ce = F.cross_entropy(logits, targets, reduction=\"none\")\n        pt = torch.exp(-ce)\n        return (self.alpha * (1 - pt) ** self.gamma * ce).mean()\n\nloss_fn = FocalLoss(gamma=2.0, alpha=1.0)\n\n# 9. Metrics with per-class breakdown\ndef compute_metrics(pred):\n    labels = pred.label_ids\n    preds  = np.argmax(pred.predictions, axis=1)\n    acc    = accuracy_score(labels, preds)\n    p, r, f1, _ = precision_recall_fscore_support(\n        labels, preds, labels=[0,1,2], zero_division=0\n    )\n    return {\n        \"accuracy\":      acc,\n        \"precision_-1\":  p[0], \"recall_-1\":  r[0], \"f1_-1\":  f1[0],\n        \"precision_0\":   p[1], \"recall_0\":   r[1], \"f1_0\":   f1[1],\n        \"precision_1\":   p[2], \"recall_1\":   r[2], \"f1_1\":   f1[2],\n    }\n\n# 10. Training arguments with warmup_steps & cosine schedule\ntraining_args = TrainingArguments(\n    output_dir             = \"./results\",\n    eval_strategy          = \"epoch\",\n    save_strategy          = \"epoch\",\n    load_best_model_at_end = True,\n    metric_for_best_model  = \"f1_1\",          # track f1 for label=2 (“correct”)\n    greater_is_better      = True,\n    per_device_train_batch_size = 16,\n    per_device_eval_batch_size  = 32,\n    learning_rate          = 2e-5,\n    weight_decay           = 0.01,\n    warmup_steps           = 500,\n    lr_scheduler_type      = \"cosine\",\n    num_train_epochs       = 10,\n    fp16                   = True,\n    logging_dir            = \"./logs\",\n    logging_strategy       = \"epoch\",\n)\n\n# 11. Custom Trainer hooking sampler & focal loss\nclass CustomTrainer(Trainer):\n    def __init__(self, sampler=None, **kwargs):\n        super().__init__(**kwargs)\n        self.train_sampler = sampler\n\n    def get_train_dataloader(self):\n        return DataLoader(\n            self.train_dataset,\n            sampler=self.train_sampler,\n            batch_size=self.args.per_device_train_batch_size,\n            collate_fn=self.data_collator\n        )\n\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        loss = loss_fn(outputs.logits, labels.to(outputs.logits.device))\n        return (loss, outputs) if return_outputs else loss\n\ntrainer = CustomTrainer(\n    model           = model,\n    args            = training_args,\n    train_dataset   = train_ds,\n    eval_dataset    = val_ds,\n    data_collator   = data_collator,\n    compute_metrics = compute_metrics,\n    sampler         = train_sampler,\n    callbacks       = [EarlyStoppingCallback(early_stopping_patience=2)],\n)\n\n# 12. Train\ntrainer.train()\n\n# 13. Evaluate on test set\npreds_out = trainer.predict(test_ds)\ny_true = preds_out.label_ids\ny_pred = np.argmax(preds_out.predictions, axis=1)\n\nprint(\"Test Classification Report:\")\nprint(classification_report(\n    y_true, y_pred,\n    target_names=[\"Incorrect(-1)\",\"Partial(0)\",\"Correct(1)\"]\n))\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_true, y_pred))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T22:45:02.501367Z","iopub.execute_input":"2025-05-10T22:45:02.501533Z","iopub.status.idle":"2025-05-10T22:51:19.955332Z","shell.execute_reply.started":"2025-05-10T22:45:02.501517Z","shell.execute_reply":"2025-05-10T22:51:19.954681Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 22:45:25.261513: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746917125.626942      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746917125.741485      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc285dfebe9e46da955120f1c804fecb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b75677374e746bfba668e0e9dc11d7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0596ee98a5eb41b88c27c7ccabd21322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2dd71b3eb34afc831072890335aee7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a6d21e822cf4a0d95ef0e4c0ff2663d"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca0d1e5ede8d43a988a173f0abc3dec3"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1130' max='1130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1130/1130 04:16, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision -1</th>\n      <th>Recall -1</th>\n      <th>F1 -1</th>\n      <th>Precision 0</th>\n      <th>Recall 0</th>\n      <th>F1 0</th>\n      <th>Precision 1</th>\n      <th>Recall 1</th>\n      <th>F1 1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.494400</td>\n      <td>0.475285</td>\n      <td>0.446667</td>\n      <td>0.460621</td>\n      <td>0.965000</td>\n      <td>0.623586</td>\n      <td>0.178571</td>\n      <td>0.384615</td>\n      <td>0.243902</td>\n      <td>1.000000</td>\n      <td>0.012658</td>\n      <td>0.025000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.358400</td>\n      <td>0.169319</td>\n      <td>0.844444</td>\n      <td>0.902857</td>\n      <td>0.790000</td>\n      <td>0.842667</td>\n      <td>0.189189</td>\n      <td>0.538462</td>\n      <td>0.280000</td>\n      <td>0.903361</td>\n      <td>0.907173</td>\n      <td>0.905263</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.149700</td>\n      <td>0.177719</td>\n      <td>0.875556</td>\n      <td>0.980892</td>\n      <td>0.770000</td>\n      <td>0.862745</td>\n      <td>0.205882</td>\n      <td>0.538462</td>\n      <td>0.297872</td>\n      <td>0.899614</td>\n      <td>0.983122</td>\n      <td>0.939516</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.085300</td>\n      <td>0.203756</td>\n      <td>0.888889</td>\n      <td>0.873786</td>\n      <td>0.900000</td>\n      <td>0.886700</td>\n      <td>0.190476</td>\n      <td>0.307692</td>\n      <td>0.235294</td>\n      <td>0.968610</td>\n      <td>0.911392</td>\n      <td>0.939130</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.073700</td>\n      <td>0.168039</td>\n      <td>0.915556</td>\n      <td>0.920000</td>\n      <td>0.920000</td>\n      <td>0.920000</td>\n      <td>0.333333</td>\n      <td>0.307692</td>\n      <td>0.320000</td>\n      <td>0.941176</td>\n      <td>0.945148</td>\n      <td>0.943158</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.066800</td>\n      <td>0.405351</td>\n      <td>0.928889</td>\n      <td>0.983516</td>\n      <td>0.895000</td>\n      <td>0.937173</td>\n      <td>0.500000</td>\n      <td>0.307692</td>\n      <td>0.380952</td>\n      <td>0.903846</td>\n      <td>0.991561</td>\n      <td>0.945674</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.095900</td>\n      <td>0.420019</td>\n      <td>0.937778</td>\n      <td>0.973684</td>\n      <td>0.925000</td>\n      <td>0.948718</td>\n      <td>0.375000</td>\n      <td>0.230769</td>\n      <td>0.285714</td>\n      <td>0.928571</td>\n      <td>0.987342</td>\n      <td>0.957055</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.048700</td>\n      <td>0.372202</td>\n      <td>0.942222</td>\n      <td>0.968912</td>\n      <td>0.935000</td>\n      <td>0.951654</td>\n      <td>0.500000</td>\n      <td>0.230769</td>\n      <td>0.315789</td>\n      <td>0.932271</td>\n      <td>0.987342</td>\n      <td>0.959016</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.042800</td>\n      <td>0.345710</td>\n      <td>0.942222</td>\n      <td>0.979058</td>\n      <td>0.935000</td>\n      <td>0.956522</td>\n      <td>0.428571</td>\n      <td>0.230769</td>\n      <td>0.300000</td>\n      <td>0.928571</td>\n      <td>0.987342</td>\n      <td>0.957055</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.025600</td>\n      <td>0.364463</td>\n      <td>0.935556</td>\n      <td>0.978723</td>\n      <td>0.920000</td>\n      <td>0.948454</td>\n      <td>0.375000</td>\n      <td>0.230769</td>\n      <td>0.285714</td>\n      <td>0.921260</td>\n      <td>0.987342</td>\n      <td>0.953157</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test Classification Report:\n               precision    recall  f1-score   support\n\nIncorrect(-1)       0.93      0.90      0.92     16614\n   Partial(0)       0.12      0.20      0.15       320\n   Correct(1)       0.90      0.91      0.91     13532\n\n     accuracy                           0.90     30466\n    macro avg       0.65      0.67      0.66     30466\n weighted avg       0.91      0.90      0.90     30466\n\nConfusion Matrix:\n[[15003   343  1268]\n [  124    65   131]\n [ 1051   120 12361]]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}