{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11631813,"sourceType":"datasetVersion","datasetId":7297994}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom transformers import (\n    BertTokenizerFast,\n    BertForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    EarlyStoppingCallback\n)\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:30:10.726655Z","iopub.execute_input":"2025-05-01T01:30:10.726969Z","iopub.status.idle":"2025-05-01T01:30:38.386429Z","shell.execute_reply.started":"2025-05-01T01:30:10.726946Z","shell.execute_reply":"2025-05-01T01:30:38.385846Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 01:30:24.382926: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746063024.602058      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746063024.665034      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# 2. Load data with encoding\ntrain_df = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/train.csv\", encoding=\"latin1\")\ntest_df  = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/test.csv\",  encoding=\"latin1\")\n\n\n# 3. Map labels \nlabel_map = {-1: 0, 0: 1, 1: 2}\ntrain_df[\"label_id\"] = train_df[\"label\"].map(label_map)\ntest_df[\"label_id\"]  = test_df[\"label\"].map(label_map)\n\n\n# 4. Text normalization\ndef normalize(text):\n    text = str(text)\n    text = re.sub(r\"\\s+\", \" \", text)           # collapse whitespace\n    text = re.sub(r\"[^A-Za-z0-9 ]\", \"\", text)  # remove punctuation\n    return text.strip().lower()\n\n\n# 5. Prepare (Question,Reference,Response) triples and labels\ntriples = [\n    (normalize(q), normalize(r), normalize(s))\n    for q,r,s in zip(train_df[\"Question\"], train_df[\"CorrectAnswer\"], train_df[\"Response\"])\n]\nlabels  = train_df[\"label_id\"].tolist()\n\n\n# 6. Stratified 90/10 train/validation split\ntrain_data, val_data, train_labels, val_labels = train_test_split(\n    triples, labels,\n    test_size=0.1,\n    random_state=42,\n    stratify=labels\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:30:38.387531Z","iopub.execute_input":"2025-05-01T01:30:38.388041Z","iopub.status.idle":"2025-05-01T01:30:38.555528Z","shell.execute_reply.started":"2025-05-01T01:30:38.388023Z","shell.execute_reply":"2025-05-01T01:30:38.554944Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# 7. Tokenizer & Dataset\ntokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n\nclass SADataset(torch.utils.data.Dataset):\n    def __init__(self, data, labels, tokenizer, max_length=128):\n        self.data = data\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        q, ref, rsp = self.data[idx]\n        enc = self.tokenizer(\n            q + \" \" + ref,    # segment A\n            rsp,              # segment B\n            truncation=True,\n            padding=\"max_length\",\n            max_length=self.max_length\n        )\n        item = {k: torch.tensor(v) for k, v in enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\ntrain_ds = SADataset(train_data, train_labels, tokenizer)\nval_ds   = SADataset(val_data,   val_labels,   tokenizer)\n\n# Prepare test set\nq_test = [normalize(q) for q in test_df[\"Question\"]]\nr_test = [normalize(r) for r in test_df[\"CorrectAnswer\"]]\ns_test = [normalize(s) for s in test_df[\"Response\"]]\ny_test = test_df[\"label_id\"].tolist()\ntest_ds = SADataset(list(zip(q_test, r_test, s_test)), y_test, tokenizer)\n\n\n# 8. Compute class weights for focal loss\ncw = compute_class_weight(\"balanced\", classes=[0,1,2], y=train_labels)\nalpha = torch.tensor(cw, dtype=torch.float, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:30:38.556381Z","iopub.execute_input":"2025-05-01T01:30:38.556934Z","iopub.status.idle":"2025-05-01T01:30:40.035107Z","shell.execute_reply.started":"2025-05-01T01:30:38.556908Z","shell.execute_reply":"2025-05-01T01:30:40.034546Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd4384a59e34f8cab6eb1f5a8e8d14e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe26077809d34654b4cfd1ccafae87cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f7fcaea3dd141279da5414e02218d4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"605c62e96d5447318f1affe28bb06e25"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# 9. Focal Loss implementation\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha, gamma=2.0, reduction=\"mean\"):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, labels):\n        ce = F.cross_entropy(logits, labels, weight=self.alpha, reduction=\"none\")\n        pt = torch.exp(-ce)\n        loss = (1 - pt) ** self.gamma * ce\n        return loss.mean() if self.reduction==\"mean\" else loss.sum()\n\n\n# 10. Custom Trainer to plug in focal loss\nclass FocalTrainer(Trainer):\n    def compute_loss(\n        self,\n        model,\n        inputs,\n        return_outputs: bool = False,\n        num_items_in_batch: int = None       # <- add this parameter\n    ):\n        labels = inputs.pop(\"labels\")\n        # forward pass\n        outputs = model(**inputs)\n        logits = outputs.logits\n        # compute focal loss (alpha/gamma defined elsewhere)\n        loss = FocalLoss(alpha, gamma=2.0)(logits, labels.to(logits.device))\n        # return both loss and outputs if asked\n        return (loss, outputs) if return_outputs else loss\n\n\n# 11. Metrics: accuracy + macro-F1\ndef compute_metrics(pred):\n    preds  = pred.predictions.argmax(-1)\n    labels = pred.label_ids\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"macro_f1\": f1_score(labels, preds, average=\"macro\")\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:30:40.036325Z","iopub.execute_input":"2025-05-01T01:30:40.036566Z","iopub.status.idle":"2025-05-01T01:30:40.043641Z","shell.execute_reply.started":"2025-05-01T01:30:40.036544Z","shell.execute_reply":"2025-05-01T01:30:40.042882Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# 12. Load and prepare BERT model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    num_labels=3,\n    id2label={0:\"incorrect\",1:\"partial\",2:\"correct\"},\n    label2id={\"incorrect\":0,\"partial\":1,\"correct\":2}\n).to(device)\n\n\n# 13. TrainingArguments (transformers 4.51.1)\ntraining_args = TrainingArguments(\n    output_dir=\"./bert_sa\",\n    run_name=\"sa_grading\",\n    report_to=\"none\",                  \n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    num_train_epochs=10,\n    eval_strategy=\"steps\",\n    eval_steps=200,\n    save_strategy=\"steps\",\n    save_steps=200,\n    logging_strategy=\"steps\",\n    logging_steps=200,\n    dataloader_pin_memory=False,      \n    load_best_model_at_end=True,\n    metric_for_best_model=\"macro_f1\",\n    greater_is_better=True\n)\n\ntrainer = FocalTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n)\n\n\n# 14. Train & Validation Evaluation\ntrainer.train()\nprint(trainer.evaluate())\n\n# 15. Final Test Performance\npreds_out = trainer.predict(test_ds)\npreds     = preds_out.predictions.argmax(-1)\nprint(classification_report(y_test, preds, target_names=[\"incorrect\",\"partial\",\"correct\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T01:30:40.044336Z","iopub.execute_input":"2025-05-01T01:30:40.044548Z","iopub.status.idle":"2025-05-01T01:39:59.354218Z","shell.execute_reply.started":"2025-05-01T01:30:40.044528Z","shell.execute_reply":"2025-05-01T01:39:59.353624Z"}},"outputs":[{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789188ecd407441eafd03b460ab2c173"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1270' max='1270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1270/1270 06:37, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Macro F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>200</td>\n      <td>0.600700</td>\n      <td>0.519410</td>\n      <td>0.951111</td>\n      <td>0.643022</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.382400</td>\n      <td>0.492994</td>\n      <td>0.933333</td>\n      <td>0.632456</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.287700</td>\n      <td>0.463810</td>\n      <td>0.946667</td>\n      <td>0.772704</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.150200</td>\n      <td>0.348241</td>\n      <td>0.942222</td>\n      <td>0.731838</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.094000</td>\n      <td>0.458349</td>\n      <td>0.955556</td>\n      <td>0.768096</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.037100</td>\n      <td>0.414171</td>\n      <td>0.946667</td>\n      <td>0.776282</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.4141712486743927, 'eval_accuracy': 0.9466666666666667, 'eval_macro_f1': 0.7762820512820513, 'eval_runtime': 1.204, 'eval_samples_per_second': 186.876, 'eval_steps_per_second': 6.644, 'epoch': 10.0}\n              precision    recall  f1-score   support\n\n   incorrect       0.92      0.92      0.92     16614\n     partial       0.08      0.35      0.13       320\n     correct       0.94      0.86      0.90     13532\n\n    accuracy                           0.89     30466\n   macro avg       0.65      0.71      0.65     30466\nweighted avg       0.92      0.89      0.90     30466\n\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}