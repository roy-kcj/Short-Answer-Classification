{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#!/usr/bin/env python\nimport os, random\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom datasets import Dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    set_seed,\n    EarlyStoppingCallback\n)\nfrom peft import LoraConfig, get_peft_model, TaskType\n\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# 1. Reproducibility\nSEED = 42\nset_seed(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# 2. Load & clean CSVs\ntrain_df = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/train.csv\", encoding=\"latin1\")\ntest_df  = pd.read_csv(\"/kaggle/input/5790finalproj-dataset/test.csv\",  encoding=\"latin1\")\nfor df in (train_df, test_df):\n    for col in [\"Question\",\"Response\",\"CorrectAnswer\"]:\n        df[col] = df[col].fillna(\"\").astype(str)\n\n# 3. Map labels\nlabel_map = {-1:0, 0:1, 1:2}\ntrain_df[\"label\"] = train_df[\"label\"].map(label_map)\ntest_df[\"label\"]  = test_df[\"label\"].map(label_map)\n\n# 4. Stratified split 80/20 on train\ntrain_pd, val_pd = train_test_split(\n    train_df, test_size=0.2,\n    stratify=train_df[\"label\"],\n    random_state=SEED\n)\n\n# 5. To HF Dataset\ndef df_to_ds(df):\n    ds = Dataset.from_pandas(df.reset_index(drop=True))\n    drop = [c for c in [\"index\",\"Experiment\",\"Topic\",\"ID\"] if c in ds.column_names]\n    return ds.remove_columns(drop)\n\ntrain_ds = df_to_ds(train_pd)\nval_ds   = df_to_ds(val_pd)\ntest_ds  = df_to_ds(test_df)\n\n# 6. Tokenizer & preprocess\nMODEL = \"microsoft/deberta-v3-large\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\n\ndef preprocess(batch):\n    texts = [\n        f\"{q} [SEP] {r} [SEP] {c}\"\n        for q,r,c in zip(batch[\"Question\"], batch[\"Response\"], batch[\"CorrectAnswer\"])\n    ]\n    enc = tokenizer(texts, truncation=True, max_length=256)\n    enc[\"labels\"] = batch[\"label\"]\n    return enc\n\ntrain_ds = train_ds.map(preprocess, batched=True, remove_columns=train_ds.column_names)\nval_ds   = val_ds.map(preprocess,   batched=True, remove_columns=val_ds.column_names)\ntest_ds  = test_ds.map(preprocess,  batched=True, remove_columns=test_ds.column_names)\n\n# 7. Data collator\ndata_collator = DataCollatorWithPadding(tokenizer)\n\n# 8. Model + LoRA\nbase = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=3)\nlora_cfg = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=8, lora_alpha=32, lora_dropout=0.1,\n    target_modules=[\"query_proj\",\"key_proj\",\"value_proj\",\"dense\"]\n)\nmodel = get_peft_model(base, lora_cfg)\nmodel.gradient_checkpointing_enable()\n\n# 9. Class weights\ncounts = np.bincount(train_ds[\"labels\"], minlength=3)\ncw = torch.tensor(1.0/counts, dtype=torch.float)\ncw /= cw.sum()\n\n# 10. Custom Trainer\nclass WeightedTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss_fct = torch.nn.CrossEntropyLoss(weight=cw.to(logits.device))\n        loss = loss_fct(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# 11. TrainingArguments\nargs = TrainingArguments(\n    output_dir=\"out\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    gradient_accumulation_steps=2,\n    learning_rate=3e-5,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    logging_steps=100,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_macro_f1\",\n    fp16=True\n)\n\n# 12. Metrics\ndef compute_metrics(p):\n    preds = np.argmax(p.predictions, axis=-1)\n    return {\n        \"accuracy\":    accuracy_score(p.label_ids, preds),\n        \"macro_f1\":    f1_score(p.label_ids, preds, average=\"macro\"),\n        \"weighted_f1\": f1_score(p.label_ids, preds, average=\"weighted\")\n    }\n\n# 13. Trainer\ntrainer = WeightedTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=val_ds,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n)\n\n# 14. Train & eval\nif __name__ == \"__main__\":\n    trainer.train()\n    trainer.save_model(\"best_model\")\n\n    out = trainer.predict(test_ds)\n    print(\"Test metrics:\", compute_metrics(out))\n    y_true = out.label_ids\n    y_pred = np.argmax(out.predictions, axis=-1)\n    print(classification_report(\n        y_true, y_pred,\n        target_names=[\"Incorrect(-1)\",\"Partial(0)\",\"Correct(1)\"]\n    ))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-12T20:01:01.132008Z","iopub.execute_input":"2025-05-12T20:01:01.132241Z","iopub.status.idle":"2025-05-12T20:15:52.498517Z","shell.execute_reply.started":"2025-05-12T20:01:01.132217Z","shell.execute_reply":"2025-05-12T20:15:52.497686Z"}},"outputs":[{"name":"stderr","text":"2025-05-12 20:01:18.330056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747080078.607288      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747080078.679509      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ab5365654f44deb6bc64fa0dfe7488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/580 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d844d56df6d54d91bc28de7216976bcb"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd162aaf90164ca191de1fc3e2ac717b"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f60b24e91d24321bc6d57b657b16a81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/450 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5238c55dd4c24303a72eed0abde1b5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/30466 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35590636d2024fca9b734c477230c711"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa53d080d7e1425a80404cc605f011c5"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/874M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7915b6a035d14f24b75916af89cec8a1"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_31/854096571.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n  trainer = WeightedTrainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [140/140 03:36, Epoch 4/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Macro F1</th>\n      <th>Weighted F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.114925</td>\n      <td>0.444444</td>\n      <td>0.205128</td>\n      <td>0.273504</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.097582</td>\n      <td>0.444444</td>\n      <td>0.205128</td>\n      <td>0.273504</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.087892</td>\n      <td>0.446667</td>\n      <td>0.221702</td>\n      <td>0.300207</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>1.091300</td>\n      <td>1.086728</td>\n      <td>0.446667</td>\n      <td>0.229802</td>\n      <td>0.313413</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"Test metrics: {'accuracy': 0.5475283923061773, 'macro_f1': 0.26500331844628927, 'weighted_f1': 0.4237862140980383}\n               precision    recall  f1-score   support\n\nIncorrect(-1)       0.55      0.96      0.70     16614\n   Partial(0)       0.00      0.00      0.00       320\n   Correct(1)       0.52      0.05      0.10     13532\n\n     accuracy                           0.55     30466\n    macro avg       0.36      0.34      0.27     30466\n weighted avg       0.53      0.55      0.42     30466\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}